* Localization

If you need to update the translations in the UI, you have two ways to
do that, globally or locally for a single site.

** Global

GNU gettext message catalogues (.po files) are located at
lib/AmuseWikiFarm/I18N/xx.po, where xx is the language code. Changes
done here shouldn't be site specific and if you improve the
translations, please send the updated .po file to me
(melmothx@gmail.com) or fork the amusewiki repo and do a pull request,
making sure you're aligned to the latest master branch.

** Local lexicon.json

You can override the UI translations and provide them for the
categories (if your site is multilingual or if you use codes instead
of strings) using the local file
repo/<site_id>/site_files/lexicon.json.

The format is as follows (plain JSON):

  {
     "term" : {
        "hr" : "Term in croatian",
        "en" : "Term in english",
        "it" : "Term in italian"
     },
     "another term [_1]" : {
        "hr" : "Another term in croatian with an argument [_1]",
        "en" : "Another term in english with an argument [_1]",
        "it" : "Another term in italian with an argumetn [_1]"
     }
  }

The JSON must be correct, otherwise you will not get any translation
out of that. You can use arguments for translations with the [_1],
[_2] syntax.

Strings are expected to be unescaped (html-wise). The HTML is escaped
back when passed to the template.

To validate the lexicon file prior to upload, use this (somehow
longish) one-liner:

perl -MJSON -MData::Dumper \
     -e 'open (my $fh, "<:encoding(UTF-8)", "site_files/lexicon.json");
         $/ = undef; my $json = <$fh>;
            print Data::Dumper::Dumper(JSON::from_json($json));'

If you see the dump of the data structures (an hashref of hashrefs),
you're good. Otherwise fix the json and retry.

